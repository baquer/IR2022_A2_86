{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment2_Q3.ipynb","provenance":[],"authorship_tag":"ABX9TyNZcRmFAGa2D1DjqTIQq1ul"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LN9kEJmczImd","executionInfo":{"status":"ok","timestamp":1649527677961,"user_tz":-330,"elapsed":4099,"user":{"displayName":"Lovepreet Singh","userId":"04827031498960608840"}},"outputId":"9767d53e-4569-437a-f03c-f61e07af2dcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from nltk.corpus import stopwords\n","from collections import Counter\n","from zipfile import ZipFile\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from random import shuffle\n","import os\n","import operator\n","import nltk\n","import re\n","import string\n"],"metadata":{"id":"TOTRPZtEzb2x","executionInfo":{"status":"ok","timestamp":1649527680679,"user_tz":-330,"elapsed":2724,"user":{"displayName":"Lovepreet Singh","userId":"04827031498960608840"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OE_c5So05gH","executionInfo":{"status":"ok","timestamp":1649527680680,"user_tz":-330,"elapsed":9,"user":{"displayName":"Lovepreet Singh","userId":"04827031498960608840"}},"outputId":"910faf86-5e12-473a-fa37-d717027c3f12"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/Assignment-2-IR/')"],"metadata":{"id":"HSTmpT2Nzir-","executionInfo":{"status":"ok","timestamp":1649527680681,"user_tz":-330,"elapsed":8,"user":{"displayName":"Lovepreet Singh","userId":"04827031498960608840"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["zip_ref = ZipFile(\"20_newsgroups.zip\", 'r')\n","zip_ref.extractall()\n","zip_ref.close()"],"metadata":{"id":"vQmfdiB2zkwe","executionInfo":{"status":"ok","timestamp":1649516706985,"user_tz":-330,"elapsed":268597,"user":{"displayName":"Lovepreet Singh","userId":"04827031498960608840"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class_list=[]\n","list_labels=['talk.politics.misc','comp.graphics',  'sci.space','rec.sport.hockey', 'sci.med' ] \n","list_files=[]\n","for label in list_labels:\n","  for path_root, _, files_list in os.walk(str(os.getcwd())+'/'+\"20_newsgroups\"+'/'+str(label)):\n","      for classFile in files_list:\n","          path_File = os.path.join(path_root, classFile)\n","          list_files.append(path_File)\n","          class_list.append(label)\n","        \n"],"metadata":{"id":"Cfi2x-v52k9n","executionInfo":{"status":"ok","timestamp":1649527680682,"user_tz":-330,"elapsed":8,"user":{"displayName":"Lovepreet Singh","userId":"04827031498960608840"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["len(class_list),len(list_files)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xj1j9Y4o9aq4","executionInfo":{"status":"ok","timestamp":1649527687862,"user_tz":-330,"elapsed":646,"user":{"displayName":"Lovepreet Singh","userId":"04827031498960608840"}},"outputId":"55a36b85-f13b-45f7-9e6f-9ef3ff3fed02"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5000, 5000)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from typing import TextIO\n","def lower_case(text):\n","    lower_case_text = text.str.lower()\n","    return lower_case_text\n","\n","def stop_word(text):\n","  sentence = []\n","  stop_words = set(stopwords.words(\"english\"))\n","  for w in text:\n","        if w not in stop_words:\n","            sentence.append(w)\n","        else:\n","          continue\n","  return \" \".join(sentence)\n","\n","def remove_punc(text):\n","  punc_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n","  text = punc_tokenizer.tokenize(text)\n","  return text\n","\n","def conversion(text):\n","  text = np.char.replace(text, \"0\", \" zero \")\n","  text = np.char.replace(text, \"1\", \" one \")\n","  text = np.char.replace(text, \"2\", \" two \")\n","  text = np.char.replace(text, \"3\", \" three \")\n","  text = np.char.replace(text, \"4\", \" four \")\n","  text = np.char.replace(text, \"5\", \" five \")\n","  text = np.char.replace(text, \"6\", \" six \")\n","  text = np.char.replace(text, \"7\", \" seven \")\n","  text = np.char.replace(text, \"8\", \" eight \")\n","  text = np.char.replace(text, \"9\", \" nine \")\n","  return text\n","\n","def num2Words(text):\n","  return [word for word in conversion(text) ]\n","\n","def lemmatization(text):\n","  lemmatizer = WordNetLemmatizer()\n","  tokenizer=nltk.tokenize.WhitespaceTokenizer()\n","  text = [lemmatizer.lemmatize(word) for word in tokenizer.tokenize(text)]\n","  return text \n"],"metadata":{"id":"wgbjKeJv9xP3","executionInfo":{"status":"ok","timestamp":1649527688615,"user_tz":-330,"elapsed":3,"user":{"displayName":"Lovepreet Singh","userId":"04827031498960608840"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def preProcessText(text):\n","  text =lower_case(text)\n","  text = text.str.replace('\\[.*?\\]', '')\n","  text = text.str.replace('https?://\\S+|www\\.\\S+', '')\n","  text = text.str.replace('<.*?>+', '')\n","  text = text.str.replace('\\n', '')\n","  text = text.str.replace('[%s]' % re.escape(string.punctuation), '')\n","  text = text.str.replace('\\n', '')\n","  text = stop_word(text)\n","  text = lemmatization(text)\n","  return \" \".join(text)"],"metadata":{"id":"k2DdIukM91BP","executionInfo":{"status":"ok","timestamp":1649527691611,"user_tz":-330,"elapsed":454,"user":{"displayName":"Lovepreet Singh","userId":"04827031498960608840"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["list_docs = []\n","for path in list_files:\n","  readFile = open(path, 'r',encoding='cp1250')\n","  txt = readFile.read().strip()\n","  readFile.close()\n","  list_docs.append(txt)\n","df_docs = pd.DataFrame([list_docs,class_list]).T\n","df_docs[0] = preProcessText(df_docs[0])\n","df_docs.to_pickle(\"df_docs\")"],"metadata":{"id":"Hh2icGc3HquK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_docs"],"metadata":{"id":"D_A2VT04NisX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def createClassBasedWords(data,class_list):\n","  temp = {}\n","  for j in range(len(data)):\n","    if class_list[j] not in temp.keys():\n","      temp[class_list[j]] = data[j]\n","    else:\n","      temp[class_list[j]] += data[j]\n","  return temp\n","\n","def countWordPerClass(class_dict):\n","  word_dict = {}\n","  for classT in class_dict:\n","    words = set(class_dict[classT])\n","    for word in words:\n","      if word in word_dict.keys():\n","        word_dict[word] +=1\n","      else:\n","        word_dict[word] =1\n","  print(word_dict)\n","  return word_dict\n","\n","def tf_Icf(words,class_dict,word_dict):\n","  count_words = Counter(words)\n","  tf_icf = {}\n","  for word in set(words):\n","    tf_value = count_words[word]\n","    icf_value = np.log(len(class_dict)/word_dict[word])\n","    tf_icf[word] = tf_value*icf_value\n","  print(tf_icf)\n","  return tf_icf\n","\n","def fit(data):\n","  temp = createClassBasedWords(data[0].tolist(),data[0].tolist())\n","  words_list = []\n","  for i in temp:\n","    words_list += temp[i]\n","  word_dict = countWordPerClass(temp)\n","  tf_icf = tf_Icf(words_list,temp,word_dict)\n","  tf_icf_sorted = sorted(tf_icf.items(), key=lambda t: t[1],reverse=True)\n","  unique_words = [word_tf_icf[0] for word_tf_icf in tf_icf_sorted[:int(len(tf_icf_sorted)*10/100)]]\n","  wordFreCl= {}\n","  numWorCl={}\n"],"metadata":{"id":"QU1zPVWwVCOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ratio_list = [0.5,0.7,0.8]\n","tmp_list = []\n","for ratio in ratio_list:\n","  data_train = df_docs.sample(frac=ratio,random_state=42)\n","  data_test = df_docs[~df_docs.index.isin(data_train.index)]\n","  fit(data_train)\n"],"metadata":{"id":"Cf82qZJKQI49"},"execution_count":null,"outputs":[]}]}